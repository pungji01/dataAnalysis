{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fJxi6vYZ4rdB",
   "metadata": {
    "id": "fJxi6vYZ4rdB"
   },
   "source": [
    "# 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h0qaUXpk4sIv",
   "metadata": {
    "id": "h0qaUXpk4sIv"
   },
   "source": [
    "## 4. 웹 크롤링으로 데이터 수집 - 정적 웹크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XxxWGl5l5cg7",
   "metadata": {
    "id": "XxxWGl5l5cg7"
   },
   "source": [
    "### #그래프에서 한글사용하는 방법\n",
    "- **(코랩에서)한글폰트 설치하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FIjAaO5w5dFZ",
   "metadata": {
    "id": "FIjAaO5w5dFZ"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf\n",
    "\n",
    "# 코랩에서 위 코드를 실행시킨 후  반드시 코랩 메뉴: \"런타임>런타임 다시 시작\" 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AVd9eWKQ5geE",
   "metadata": {
    "id": "AVd9eWKQ5geE"
   },
   "source": [
    "- **한글 폰트 지정하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yrr8KJBG5hAD",
   "metadata": {
    "id": "Yrr8KJBG5hAD"
   },
   "outputs": [],
   "source": [
    "# 코랩에서 한글 폰트 종류와 이름이 win과 다를 수 있다!!!\n",
    "# 코랩: NanumGothic, 윈도우: Malgun Gothic\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.family': 'Malgun Gothic',\n",
    "                     'font.size': 12,\n",
    "                     'figure.figsize': (6, 4),\n",
    "                     'axes.unicode_minus':  False }) # 폰트 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xKVwMm3T4sDO",
   "metadata": {
    "id": "xKVwMm3T4sDO"
   },
   "source": [
    "### 1) 웹 크롤링 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7e431",
   "metadata": {
    "id": "84e7e431"
   },
   "source": [
    "### 라이브러리 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e2b87",
   "metadata": {
    "id": "8c4e2b87"
   },
   "outputs": [],
   "source": [
    "# 정적 크롤링을 위한 requests 설치\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc94d8",
   "metadata": {
    "id": "10fc94d8"
   },
   "outputs": [],
   "source": [
    "# HTML과 XML 문서를 파싱하기 위한 파이썬 패키지\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qWFf0C8852IZ",
   "metadata": {
    "id": "qWFf0C8852IZ"
   },
   "outputs": [],
   "source": [
    "# 동적 크롤링을 위한 셀레니움 설치\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5uqxJ2Xf6Wex",
   "metadata": {
    "id": "5uqxJ2Xf6Wex"
   },
   "outputs": [],
   "source": [
    "# 동적 크롤링을 위한 크롬드라이버 자동설치 라이브러리 설치\n",
    "!pip install chromedriver-autoinstaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8480f3",
   "metadata": {
    "id": "3f8480f3"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import selenium\n",
    "print(requests.__version__)\n",
    "print(bs4.__version__)\n",
    "print(selenium.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mJ-v8Qjk5CPp",
   "metadata": {
    "id": "mJ-v8Qjk5CPp"
   },
   "source": [
    "### 2) 정적 크롤링(스크래핑)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0d7af",
   "metadata": {
    "id": "aed0d7af"
   },
   "source": [
    "#### 1.웹 페이지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sIxkxPRg42rD",
   "metadata": {
    "id": "sIxkxPRg42rD"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "requests.get(\"https://www.naver.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0BRs8FHc78t1",
   "metadata": {
    "id": "0BRs8FHc78t1"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://google.com\")\n",
    "\n",
    "# 응답 상태\n",
    "print('#응답 상태: ', response.status_code)\n",
    "\n",
    "# 응답 바이너리 원문\n",
    "print('#응답 바이너리 원문: ', response.content)\n",
    "\n",
    "# 응답 UTF-8로 인코딩된 문자열\n",
    "print('#응답 UTF-8로 인코딩된 문자열: ', response.text)\n",
    "\n",
    "# 응답 헤더\n",
    "print('#응답 헤더: ', response.headers)\n",
    "\n",
    "# 응답 헤더: 콘텐트 유형\n",
    "print('#응답 헤더유형: ', response.headers['Content-Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b7a41",
   "metadata": {
    "id": "c93b7a41"
   },
   "source": [
    "#### 2.웹 페이지에서 정보 추출하기\n",
    "- BeautifulSoup 라이브러리 사용하여 웹페이지에서 정보 추출하기\n",
    "- 네이버 메인(https://www.naver.com/) > 검색어 입력(**눈물의여왕**) > 검색된 결과 페이지에서 **제목**만 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda0fb4",
   "metadata": {
    "id": "0bda0fb4"
   },
   "source": [
    "#### [예제] : Text 제목 정보 1개 가져오기 in (정적인 페이지)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mm1JMSRiBMcz",
   "metadata": {
    "id": "Mm1JMSRiBMcz"
   },
   "source": [
    "* 뉴스 제목이 있는 위치 찾는 방법\n",
    "    1. 크롬브라우저를 연다.\n",
    "    2. Target URL =https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EB%88%88%EB%AC%BC%EC%9D%98%EC%97%AC%EC%99%95 페이지를 연결한다.\n",
    "    3. 키보드에서 F12 키를 눌러 크롬 개발자 도구를 연다\n",
    "    4. 개발자 도구에서 마우스로 영역선택( )메뉴를 클릭하고 마우스를 뉴스 제목 부분에 놓고 클릭한다.\n",
    "    5.마우스 오른쪽 버튼을 클릭하고 메뉴(Copy > copy selector)를 선택한다.\n",
    "    6. 5에서 복사된 뉴스 제목이 있는 영역으 tag를 코드에 붙여넣는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea1900",
   "metadata": {
    "id": "75ea1900"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EB%88%88%EB%AC%BC%EC%9D%98%EC%97%AC%EC%99%95'\n",
    "print(url)\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.select_one('#sp_nws1 > div.news_wrap.api_ani_send > div > div.news_contents > a.news_tit')\n",
    "    print(title)\n",
    "    print(f\"제목: {title.attrs['title']}\")  # 제목 추출 : title.get_text()\n",
    "else :\n",
    "    print(response.status_code)\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf83f7",
   "metadata": {
    "id": "c8bf83f7"
   },
   "source": [
    "#### [예제] Text 정보 여러 개 가져오기 in (정적인 페이지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbf9b8",
   "metadata": {
    "id": "39fbf9b8"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print('페이지 정보 추출하기-----')\n",
    "url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=%EB%88%88%EB%AC%BC%EC%9D%98%EC%97%AC%EC%99%95'\n",
    "print(url)\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "t_list, d_list, link_list = [], [], []\n",
    "if response.status_code == 200:\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    ul = soup.select_one('ul.list_news') #공백에 있을 경우 . 사용\n",
    "    # 제목, 링크\n",
    "    titles = ul.select('li > div > div > div.news_contents > a.news_tit')\n",
    "    for title in titles:\n",
    "        t_list.append(title.attrs['title'])\n",
    "        link_list.append(title.attrs['href'])\n",
    "#         print(title.get_text())\n",
    "    # 상세 설명\n",
    "    titles = ul.select('li > div > div > div.news_contents > div.news_dsc > div.dsc_wrap > a')\n",
    "    for title in titles:\n",
    "        d_list.append(title.get_text())\n",
    "else :\n",
    "    print(response.status_code)\n",
    "print('페이지 정보 추출완료-----')\n",
    "t_list, d_list, link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdbd64",
   "metadata": {
    "id": "bacdbd64"
   },
   "outputs": [],
   "source": [
    "# pandas DataFrame으로 나타내기\n",
    "import pandas as pd\n",
    "\n",
    "data = {'title': t_list, 'desc':d_list,'link':link_list}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('my_naver_news.csv', index=False) # 파일로 저장하기\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L6_vCg4fGroe",
   "metadata": {
    "id": "L6_vCg4fGroe"
   },
   "source": [
    "#### **[실습] : 할리스 커피매장 정보 수집하기**\n",
    " - 매장정보 : https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=2&sido=&gugun=&store=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c58f5cf",
   "metadata": {
    "id": "2c58f5cf"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "MAX = 100    # 총 가져올 건수\n",
    "FILE = './data/hollys_매장정보.csv'\n",
    "\n",
    "#[CODE 1]\n",
    "def hollys_store(result):\n",
    "    cnt, MAX_flag = 0, False\n",
    "    for page in range(1,59):\n",
    "#         Hollys_url = 'https://www.hollys.co.kr/store/korea/korStore.do?pageNo=%d&sido=&gugun=&store=' %page\n",
    "        Hollys_url = 'https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=%d&sido=&gugun=&store=' %page\n",
    "        print(Hollys_url)\n",
    "        html = urllib.request.urlopen(Hollys_url)\n",
    "        soupHollys = BeautifulSoup(html, 'html.parser')\n",
    "        tag_tbody = soupHollys.find('tbody')\n",
    "        for store in tag_tbody.find_all('tr'):\n",
    "            if len(store) <= 3:\n",
    "                break\n",
    "            store_td = store.find_all('td')\n",
    "            store_name = store_td[1].string\n",
    "            store_sido = store_td[0].string\n",
    "            store_address = store_td[3].string\n",
    "            store_phone = store_td[5].string\n",
    "            result.append([store_name]+[store_sido]+[store_address]\n",
    "                          +[store_phone])\n",
    "            cnt += 1\n",
    "            print(f'[{cnt:0>3}] {store_name}  \\t {store_sido}')\n",
    "            if cnt == MAX:\n",
    "                MAX_flag = True\n",
    "                break\n",
    "        if MAX_flag:\n",
    "            break\n",
    "\n",
    "    return\n",
    "\n",
    "#[CODE 0]\n",
    "def main():\n",
    "    result = []\n",
    "    print('Hollys store crawling >>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    hollys_store(result)   #[CODE 1] 호출\n",
    "    hollys_tbl = pd.DataFrame(result, columns=('store', 'sido-gu', 'address','phone'))\n",
    "    hollys_tbl.to_csv(FILE, encoding='cp949', mode='w', index=True)\n",
    "    del result[:]\n",
    "    return hollys_tbl\n",
    "\n",
    "\n",
    "df = main()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OTLFBkYIHVZJ",
   "metadata": {
    "id": "OTLFBkYIHVZJ"
   },
   "source": [
    "#### **[실습] :  텍스트 데이터를 토큰화하여 빈도수를 구하고 빈도수에 따른 워드클라우드 만들기**\n",
    "- 앞에서 수집한 할리스 커피매장의 매장 위치 정보 중 지역(**sido-gu**) 정보를 이용하여 워드 클라우드 만들기\n",
    "- 필요한 라이브러리 및 주의사항\n",
    "    - 사이킷런 CountVectorizer클래스 사용 : 단어 빈도수 추출\n",
    "    - 단, 한글에서 불용어 처리 및 가중치 처리 등 자연어 텍스트 전처리에 필요한 여러 가지 방법은 여기서 다루지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AJ1u5JwhHfjF",
   "metadata": {
    "id": "AJ1u5JwhHfjF"
   },
   "outputs": [],
   "source": [
    "# 사이킷런 설치하기\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d6e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mYKxCNg5HoGP",
   "metadata": {
    "id": "mYKxCNg5HoGP"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V5Ec5vy7Hrz5",
   "metadata": {
    "id": "V5Ec5vy7Hrz5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "\n",
    "# 말뭉치를 토큰화하여 빈도수 가져오기\n",
    "def get_wordTokenCount(corpus):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    # 말뭉치를 토큰화하기\n",
    "    vect = CountVectorizer().fit(corpus)\n",
    "    count = vect.transform(corpus).toarray().sum(axis=0)\n",
    "\n",
    "    # 토큰 빈도수로 정렬하고 토큰명 추출\n",
    "    idx = np.argsort(-count)  # 내림 정렬하여 인덱스 반환: 토큰의 인덱스\n",
    "    count = count[idx]        # 토큰의 빈도수\n",
    "    feature_name = np.array(vect.get_feature_names_out())[idx]  # 토큰값\n",
    "\n",
    "    # 빈도수 많은 순서대로 토큰명 10개만 출력\n",
    "    print(list(zip(feature_name, count))[:10])\n",
    "\n",
    "    return feature_name, count\n",
    "\n",
    "\n",
    "# 단어(토큰) 빈도수 막대 그래프 그리기\n",
    "def draw_wordTokenCountGraph(data, freq):\n",
    "    plt.bar(data, freq)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # 그래프 그림 저장히기\n",
    "    plt.savefig(f'./token_bar_graph.png')\n",
    "\n",
    "# 워드클라우드 만들기\n",
    "def make_wordcloud(feature_name, count):\n",
    "    # 한글 폰트 경로를 설정\n",
    "    # font_path = 'NanumGothic'  #/usr/share/fonts/truetype/nanum/NanumGothic.ttf  #코랩\n",
    "    font_path = 'malgun'  # C:/Windows/Fonts/                                  #window\n",
    "\n",
    "    # (토큰명, 빈도수) 딕셔너리 타입으로 변환\n",
    "    data = dict(zip(feature_name, count))\n",
    "\n",
    "    # 워드클라우드로 그래프로 시각화\n",
    "    wc = WordCloud(width = 1000, height = 600, background_color=\"black\", font_path=font_path)\n",
    "    plt.imshow(wc.generate_from_frequencies(data)) #딕셔너리\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # 이미지 파일로 저장하기\n",
    "    wc.to_file(f'./워드클라우드.png')\n",
    "\n",
    "\n",
    "# 1.텍스트 말뭉치(corpus) 데이터 지정하기\n",
    "corpus = df['sido-gu'].to_list()\n",
    "# print(corpus)\n",
    "\n",
    "# 2.말뭉치를 토큰화하여 빈도수 가져오기\n",
    "feature_name, count = get_wordTokenCount(corpus)\n",
    "\n",
    "# 3.단어(토큰) 빈도수 막대 그래프 그리기(상위 10개)\n",
    "# draw_wordTokenCountGraph(feature_name[:10], count[:10])\n",
    "\n",
    "# 3.워드 클라우드 만들기\n",
    "make_wordcloud(feature_name, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_4UpfgKjMZXm",
   "metadata": {
    "id": "_4UpfgKjMZXm"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9Hw8EjahKJEX",
   "metadata": {
    "id": "9Hw8EjahKJEX"
   },
   "source": [
    "### **[미션] : 네이버 뉴스 기사 제목을 워드클라우드로 만들기**\n",
    "앞에서 정적크롤링으로 가져왔던 네이버 뉴스 기사 제목을 워드 클라우드로 만들어 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MNidf1-MKUvz",
   "metadata": {
    "id": "MNidf1-MKUvz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./my_naver_news.csv')\n",
    "df\n",
    "\n",
    "\n",
    "# 1.텍스트 말뭉치(corpus) 데이터 지정하기\n",
    "corpus = df['title'].to_list()\n",
    "# print(corpus)\n",
    "\n",
    "# 2.말뭉치를 토큰화하여 빈도수 가져오기\n",
    "feature_name, count = get_wordTokenCount(corpus)\n",
    "\n",
    "# 3.단어(토큰) 빈도수 막대 그래프 그리기(상위 10개)\n",
    "# draw_wordTokenCountGraph(feature_name[:10], count[:10])\n",
    "\n",
    "# 3.워드 클라우드 만들기\n",
    "make_wordcloud(feature_name, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uP53g-I9Oyto",
   "metadata": {
    "id": "uP53g-I9Oyto"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd9b70",
   "metadata": {},
   "source": [
    "#### [실습문제] : 정적크롤링 방법을 이용하여 TEXT 정보 추출하여 표로 만들기\n",
    "자신이 원하는 웹 페이지를 정해서 100개 이상 텍스트 정보 추출해서 표로 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c305f0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03492102",
   "metadata": {},
   "source": [
    "----------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
